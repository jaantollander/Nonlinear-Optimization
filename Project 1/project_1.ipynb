{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff    # For computing gradients using automatic differentiation\n",
    "using LinearAlgebra \n",
    "\n",
    "## Functions to compute gradient, hessian and first derivative\n",
    "∇(f,x) = ForwardDiff.gradient(f, x);\n",
    "H(f,x)  = ForwardDiff.hessian(f, x);\n",
    "D(θ, λ) = ForwardDiff.derivative(θ, λ)\n",
    "\n",
    "## Select which line search method to use.\n",
    "@enum LS ARMIJO GOLDEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact line search : golden section\n",
    "### Parameters: \n",
    " θ: line search function\n",
    " \n",
    " a: initial lower bound\n",
    " \n",
    " b: initial upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function golden_ls(θ, a, b)\n",
    " \n",
    "    l  = 1e-7                     # Tolerance (length of uncertainty)\n",
    "    α  = 1/Base.MathConstants.φ   # φ = golden ratio. Here α ≈ 0.618\n",
    "    \n",
    "    λ  = a + (1-α)*(b - a)        # NOTE: We do not need to index a, b, λ, and μ like in the lecture 5 pseudocode\n",
    "    μ  = a + α*(b - a)            #       Instead, we can keep reusing and updating the same variables for notational convenience\n",
    "\n",
    "    θμ = θ(a + α*(b - a))         # Use this variable to compute function values Θ(μₖ₊₁) as in the pseudocode of Lecture 5\n",
    "    θλ = θ(a + (1 - α)*(b - a))   # Use this variable to compute function values Θ(λₖ₊₁) as in the pseudocode of Lecture 5\n",
    "\n",
    "    ## TODO: Implement what should be inside the while loop of Golden Section method\n",
    "    while b - a > l\n",
    "        if θλ > θμ\n",
    "            a = λ\n",
    "            λ = μ\n",
    "            μ = a + α*(b - a)\n",
    "            θλ = θμ\n",
    "            θμ = θ(μ)\n",
    "        else\n",
    "            b = μ\n",
    "            μ = λ\n",
    "            λ = a + (1 - α)*(b - a)\n",
    "            θμ = θλ\n",
    "            θλ = θ(λ)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return (a + b)/2              # Finally, the function returns the center point of the final interval\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inexact line search : Armijo rule\n",
    "### Parameters: \n",
    "\n",
    "θ: line search function\n",
    "\n",
    "λ: initial step size value (e.g. 1)\n",
    "\n",
    "α: slope reduction factor\n",
    "\n",
    "β: λ reduction factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Armijo_ls(θ, λ, α, β) \n",
    "    \n",
    "    θ₀  = θ(0)                  # Function value at zero\n",
    "    Dθ₀ = D(θ, 0)               # Derivative (slope) at zero \n",
    "    \n",
    "    ## TODO: Implement what should be inside the while loop of Armijo method\n",
    "    while θ(λ) > θ₀ + α*λ*Dθ₀   # Check termination condition\n",
    "\n",
    "        λ = β*λ\n",
    "        \n",
    "    end\n",
    "    \n",
    "    return λ\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient (descent) method\n",
    "### Parameters\n",
    "  f: function to minimize\n",
    "  \n",
    "  x: starting point\n",
    "  \n",
    "  N: maximum number of iterations\n",
    "  \n",
    "  LS: line search method (GOLDEN or ARMIJO)\n",
    "  \n",
    "  flag: the indicator of output (true - if you need the history of the iterations, false - if you need just cost and number of iterations)\n",
    "\n",
    "  ### Keywork arguments\n",
    "  \n",
    " ε: solution tolerance\n",
    " \n",
    " a: initial lower bound for Golden section method\n",
    "\n",
    " b: initial upper bound for Golden section method\n",
    " \n",
    " λ: initial step size value (e.g. 1) for Armijo's method\n",
    " \n",
    " α: slope reduction factor for Armijo's method\n",
    " \n",
    " β: λ reduction factor for Armijo's method\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Gradient(f, x, N, LS, flag; ε = tol, a = a₀, b = b₀, λ = λ₀, α = α₀, β = β₀)\n",
    "    \n",
    "    (flag == true) && (x_iter = zeros(N, length(x)) ) # if we need to save the history of iterations \n",
    "    \n",
    "    for k = 1:N               # NOTE: initial x should be set to x0\n",
    "        \n",
    "        ∇f     = ∇(f, x)      # Gradient at iteration k  \n",
    "        norm∇f = norm(∇f)     # Norm of the gradient\n",
    "        ∇f    /= norm∇f       # Normalized gradient\n",
    "        \n",
    "        if norm∇f < ε         # Stopping condition #1\n",
    "            \n",
    "            if flag == true\n",
    "                \n",
    "                return  ( x_iter[1 : k-1, :], k - 1 )  # Return  the history of cost, iterations\n",
    "            \n",
    "            else \n",
    "                \n",
    "                return ( f(x), k - 1 )                 # Return cost and iterations\n",
    "            \n",
    "            end\n",
    "        \n",
    "        end\n",
    "        \n",
    "        ## TODO: set the Gradient Descent direction\n",
    "        d = -∇f                  # Gradient method direction\n",
    "        \n",
    "        ########## START LINE SEARCH ###############\n",
    "        θ(λ) = f(x + λ*d)\n",
    "        if LS == ARMIJO \n",
    "            (λ = Armijo_ls(θ, λ, α, β))         # Call Armijo method to compute optimal step size λ \n",
    "        else\n",
    "            (λ = golden_ls(θ, a, b))            # Call Golden Section method to compute optimal step size λ \n",
    "        end \n",
    "        ############ END LINE SEARCH ###############\n",
    "        \n",
    "        ## TODO: Update the solution x at this iteration accordingly\n",
    "        x = x + λ*d               # Update solution\n",
    "        \n",
    "        (flag == true) && ( x_iter[k, :] = x ) # save the history if needed\n",
    "        \n",
    "    end\n",
    "     \n",
    "    if flag == true\n",
    "        \n",
    "        return (x_iter, N )  # Return the history of cost, iterations\n",
    "     \n",
    "     else    \n",
    "        \n",
    "        return ( f(x), N )   # Return cost, iterations  \n",
    "     \n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heavy Ball method\n",
    "### Parameters\n",
    "  f: function to minimize\n",
    "  \n",
    "  x: starting point\n",
    "  \n",
    "  N: maximum number of iterations\n",
    "  \n",
    "  LS: line search method (GOLDEN or ARMIJO)\n",
    "  \n",
    "  weight: Heavy ball weighting parameter\n",
    "  \n",
    "  flag: the indicator of output (true - if you need the history of the iterations, false - if you need just cost and number of iterations)\n",
    "  \n",
    "  ### Keywork arguments\n",
    "  \n",
    " ε: solution tolerance\n",
    " \n",
    " a: initial lower bound for Golden section method\n",
    "\n",
    " b: initial upper bound for Golden section method\n",
    " \n",
    " λ: initial step size value (e.g. 1) for Armijo's method\n",
    " \n",
    " α: slope reduction factor for Armijo's method\n",
    " \n",
    " β: λ reduction factor for Armijo's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Heavy_ball(f, x, N, LS, weight, flag; ε = tol, a = a₀, b = b₀, λ = λ₀, α = α₀, β = β₀)\n",
    "    \n",
    "    (flag == true) && (x_iter = zeros(N, length(x)) ) # if we need to save the history of iterations \n",
    "    \n",
    "    d = zeros(size(x)) \n",
    "    \n",
    "    for k = 1:N               # Main iteration loop\n",
    "        \n",
    "        ∇f     = ∇(f, x)      # Gradient at iteration k  \n",
    "        norm∇f = norm(∇f)     # Norm of the gradient\n",
    "        ∇f    /= norm∇f       # Normalized gradient\n",
    "        \n",
    "        if norm∇f < ε         # Stopping condition: norm of the gradient < tolerance\n",
    "            \n",
    "            if flag == true\n",
    "                \n",
    "                return  ( x_iter[1 : k-1, :], k - 1 )  # Return  the history of cost, iterations\n",
    "            \n",
    "            else \n",
    "                \n",
    "                return ( f(x), k - 1 )                 # Return cost and iterations\n",
    "            \n",
    "            end\n",
    "        end     \n",
    "        \n",
    "        ## TODO: set the Heavy ball direction\n",
    "        \n",
    "        d = (1 - weight)*(-∇f) + weight * d\n",
    "        \n",
    "        ########## START LINE SEARCH ###############\n",
    "        θ(λ) = f(x + λ*d)                                   # Define the line search function \n",
    "        if LS == ARMIJO \n",
    "            (λ = Armijo_ls(θ, λ, α, β))         # Call Armijo method to compute optimal step size λ \n",
    "        else\n",
    "            (λ = golden_ls(θ, a, b))            # Call Golden Section method to compute optimal step size λ \n",
    "        end  \n",
    "        ############ END LINE SEARCH ###############\n",
    "        \n",
    "        \n",
    "        ## TODO: Update the solution x at this iteration accordingly\n",
    "        x = x + λ*d\n",
    "    \n",
    "        (flag == true) && ( x_iter[k, :] = x ) # save the history if needed\n",
    "    \n",
    "    end\n",
    "    \n",
    "    if flag == true\n",
    "        \n",
    "        return (x_iter, N )  # Return the history of cost, iterations\n",
    "     \n",
    "    else    \n",
    "        \n",
    "        return ( f(x), N )   # Return cost, iterations  \n",
    "     \n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's method\n",
    "### Parameters\n",
    "  f: function to minimize\n",
    "  \n",
    "  x: starting point\n",
    "  \n",
    "  N: maximum number of iterations\n",
    "  \n",
    "  LS: line search method (GOLDEN or ARMIJO) \n",
    "  \n",
    "  flag: the indicator of output (true - if you need the history of the iterations, false - if you need just cost and number of iterations)\n",
    "  \n",
    "  ### Keywork arguments\n",
    "  \n",
    " ε: solution tolerance\n",
    " \n",
    " a: initial lower bound for Golden section method\n",
    "\n",
    " b: initial upper bound for Golden section method\n",
    " \n",
    " λ: initial step size value (e.g. 1) for Armijo's method\n",
    " \n",
    " α: slope reduction factor for Armijo's method\n",
    "  \n",
    " β: λ reduction factor for Armijo's method\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Newton(f, x, N, LS, flag; ε = tol, a = a₀, b = b₀, λ = λ₀, α = α₀, β = β₀)  \n",
    "    \n",
    "    (flag == true) && (x_iter = zeros(N, length(x)) ) # if we need to save the hostiry of iterations\n",
    "    \n",
    "    for k = 1:N                    # NOTE: initial x should be set to x0\n",
    "        \n",
    "        ∇f = ∇(f, x)               # Gradient\n",
    "        \n",
    "        if norm(∇f) < ε            # Stopping condition #1\n",
    "            \n",
    "            if flag == true\n",
    "                \n",
    "                return  ( x_iter[1 : k-1, :], k - 1 )  # Return  the history of cost, iterations\n",
    "            \n",
    "            else \n",
    "                \n",
    "                return ( f(x), k - 1 )                 # Return cost and iterations\n",
    "            \n",
    "            end\n",
    "        end\n",
    "        \n",
    "        ## TODO: set the Newton's method direction\n",
    "        d = -H(f, x)\\∇f\n",
    "        \n",
    "        ########## START LINE SEARCH ###############\n",
    "        θ(λ) = f(x + λ*d)\n",
    "        if LS == ARMIJO \n",
    "            (λ = Armijo_ls(θ, λ, α, β))         # Call Armijo method to compute optimal step size λ \n",
    "        else\n",
    "            (λ = golden_ls(θ, a, b))            # Call Golden Section method to compute optimal step size λ \n",
    "        end   \n",
    "        ############ END LINE SEARCH ###############\n",
    "        \n",
    "        ## TODO: Update the solution x at this iteration accordingly\n",
    "        x = x + λ*d                   # Move to a new point\n",
    "        \n",
    "        (flag == true) && ( x_iter[k, :] = x ) # save the history if needed\n",
    "        \n",
    "    end\n",
    "    \n",
    "    if flag == true\n",
    "        \n",
    "        return (x_iter, N )  # Return the history of cost, iterations\n",
    "     \n",
    "    else    \n",
    "        \n",
    "        return ( f(x), N )   # Return cost, iterations  \n",
    "     \n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broyden–Fletcher–Goldfarb–Shanno (BFGS) method \n",
    "### Parameters\n",
    "  f: function to minimize\n",
    "  \n",
    "  x: intial solution vector\n",
    "  \n",
    "  N: maximum number of iterations\n",
    "  \n",
    "  LS: line search method (GOLDEN or ARMIJO) \n",
    "  \n",
    "  flag: the indicator of output (true - if you need the history of the iterations, false - if you need just cost and number of iterations)\n",
    "  \n",
    "  ### Keywork arguments\n",
    "  \n",
    " ε: solution tolerance\n",
    " \n",
    " a: initial lower bound for Golden section method\n",
    "\n",
    " b: initial upper bound for Golden section method\n",
    " \n",
    " λ: initial step size value (e.g. 1) for Armijo's method\n",
    " \n",
    " α: slope reduction factor for Armijo's method\n",
    "  \n",
    " β: λ reduction factor for Armijo's method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function BFGS(f, x, N, LS, flag; ε = tol, a = a₀, b = b₀, λ = λ₀, α = α₀, β = β₀)\n",
    "    \n",
    "    (flag == true) && (x_iter = zeros(N, length(x)) ) # if we need to save the hostiry of iterations\n",
    "    \n",
    "    n  = length(x)                # Number of variables\n",
    "    B  = H(f,x)                   # Initial Hessian approximation\n",
    "    ∇f = ∇(f, x)                 # Initial gradient at x0\n",
    "    \n",
    "    for k = 1:N                   # NOTE: initial x should be set to x0\n",
    "        \n",
    "        if norm(∇f) < ε           # Stopping condition #1\n",
    "            \n",
    "            if flag == true\n",
    "                \n",
    "                return  ( x_iter[1 : k-1, :], k - 1 )  # Return the history of cost, iterations\n",
    "            \n",
    "            else \n",
    "                \n",
    "                return ( f(x), k - 1 )                 # Return cost and iterations\n",
    "            \n",
    "            end\n",
    "        end\n",
    "        \n",
    "        ## TODO: set the BFGS method direction\n",
    "        p = -B\\∇f                                           # Direction pₖ\n",
    "        \n",
    "        ########## START LINE SEARCH ###############\n",
    "        θ(λ) = f(x + λ*p)\n",
    "        if LS == ARMIJO \n",
    "            (λ = Armijo_ls(θ, λ, α, β))         # Call Armijo method to compute optimal step size λ \n",
    "        else\n",
    "            (λ = golden_ls(θ, a, b))            # Call Golden Section method to compute optimal step size λ \n",
    "        end\n",
    "        ############ END LINE SEARCH ###############\n",
    "        \n",
    "        s = λ*p                   # s = step size * direction\n",
    "        \n",
    "        ## TODO: Update the solution x at this iteration accordingly\n",
    "        x = x + λ*p                     # Update solution\n",
    "        (flag == true) && ( x_iter[k, :] = x ) # save the history if needed\n",
    "        \n",
    "        ∇fn  = ∇(f, x)          # New gradient\n",
    "        y    = ∇fn - ∇f         # Update Gradient difference\n",
    "        ∇f   = ∇fn             # Update Gradient for next iteration\n",
    "        \n",
    "        # Update the Hessian approximation\n",
    "        B = B + (y*y')/dot(y,s) - ((B*s)*(B*s)')/dot(s,B*s)\n",
    "    \n",
    "    end\n",
    "    \n",
    "    if flag == true\n",
    "        \n",
    "        return (x_iter, N )  # Return the history of cost, iterations\n",
    "     \n",
    "    else    \n",
    "        \n",
    "        return ( f(x), N )   # Return cost, iterations  \n",
    "     \n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self examination \n",
    "### Test Function 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = exp(x[1]+2*x[2]-0.2)+exp(x[1]-2*x[2]-0.2)+exp(-x[1]-0.2) \n",
    "N   = 10000             # Number of iterations \n",
    "a₀  = 0.0               # initial lower bound for Golden section method\n",
    "b₀  = 50.0              # initial upper bound for Golden section method\n",
    "α₀  = 0.01              # slope reduction factor for Armijo's method\n",
    "β₀  = 0.7               # λ reduction factor for Armijo's method\n",
    "λ₀  = 1.0               # initial step size value (e.g. 1) for Armijo's method\n",
    "tol = 1e-5              # Solution tolerance\n",
    "x   = [-4.0, -2.0]      # Starting point\n",
    "heavy_ball_weight = 0.2 # Heavy ball weighting parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Uncomment the line you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fn, kn) = Newton(f, x, N, GOLDEN, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fn, kn) = Newton(f, x, N, ARMIJO, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fb, kb) = BFGS(f, x, N, GOLDEN, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fb, kb) = BFGS(f, x, N, ARMIJO, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fg, kg) = Gradient(f, x, N, GOLDEN, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fg, kg) = Gradient(f, x, N, ARMIJO, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fh, kh) = Heavy_ball(f, x, N, GOLDEN, heavy_ball_weight, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fh, kh) = Heavy_ball(f, x, N, ARMIJO, heavy_ball_weight, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above parametrisation you should obtain the following results if your code is correct: \n",
    "\n",
    "Newton + Golden: (2.315720269874514, 4)\n",
    "\n",
    "Newton + Armijo: (2.3157202698696393, 7)\n",
    "\n",
    "BFGS + Golden: (2.315720269869641, 7)\n",
    "\n",
    "BFGS + Armijo: (2.3157202698696393, 11)\n",
    "\n",
    "Gradient + Golden: (2.3157202698697423, 7)\n",
    "\n",
    "Gradient + Armijo: (2.315720269873051, 33)\n",
    "\n",
    "Heavy ball + Godlen: (2.315720269877108, 9)\n",
    "\n",
    "Heavy ball + Armijo: (2.3157202698721155, 31)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function calls for tasks (1) - (3)\n",
    "\n",
    "If you want to measure how much time your function call takes, you can use the macro @time in front of the call. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time (f1gg, k1gg) = Gradient(f1, x1, N, GOLDEN, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "returns the time taken by the call. See http://www.pkofod.com/2017/04/24/timing-in-julia/ for more information on timing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters used for the experiments. \n",
    "\n",
    "Notice that some parameters must be set by you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to be set by you.\n",
    "## TODO: set the values for α₀, β₀, and heavy_ball_weight. \n",
    "α₀  = 0.01              # slope reduction factor for Armijo's method\n",
    "β₀  = 0.7               # λ reduction factor for Armijo's method\n",
    "heavy_ball_weight = 0.2 # Heavy ball weighting parameter\n",
    "\n",
    "\n",
    "# Predefined parameters (nothing to be changed from here onwards)\n",
    "N   = 10000             # Number of iterations \n",
    "a₀  = 0.0               # initial lower bound for Golden section method\n",
    "b₀  = 10.0              # initial upper bound for Golden section method\n",
    "λ₀  = 1.0               # initial step size value (e.g. 1) for Armijo's method\n",
    "tol = 1e-5              # Solution tolerance\n",
    "\n",
    "# Functions for Tasks (1), (2), and (3)\n",
    "f1(x) = 0.26*(x[1]^2 + x[2]^2) - 0.48*x[1]*x[2] \n",
    "f2(x) = exp(x[1] + 3*x[2] - 0.1) + exp(x[1] - 3*x[2] - 0.1) + exp(-x[1] - 0.1) \n",
    "f3(x) = (x[1]^2 + x[2] - 11)^2 + (x[1] + x[2]^2 - 7)^2\n",
    "\n",
    "x1 = [7.0, 3.0]      # Starting point for f1\n",
    "x2 = [1.0, 1.5]      # Starting point for f2\n",
    "x3 = [-2.0, 2.0]     # Starting point for f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Uncomment the line you need\n",
    "\n",
    "# Task (1)\n",
    "# (1) (cost and iterations)\n",
    "\n",
    "#(f1gg, k1gg) = Gradient(f1, x1, N, GOLDEN, false)\n",
    "#(f1ga, k1ga) = Gradient(f1, x1, N, ARMIJO, false)\n",
    "#(f1hg, k1hg) = Heavy_ball(f1, x1, N, GOLDEN, heavy_ball_weight, false)\n",
    "#(f1ha, k1ha) = Heavy_ball(f1, x1, N, ARMIJO, heavy_ball_weight, false)\n",
    "\n",
    "# (1) (history and iterations)\n",
    "\n",
    "#(x1gg, k1gg) = Gradient(f1, x1, N, GOLDEN, true)\n",
    "#(x1ga, k1ga) = Gradient(f1, x1, N, ARMIJO, true)\n",
    "#(x1hg, k1hg) = Heavy_ball(f1, x1, N, GOLDEN, heavy_ball_weight, true)\n",
    "#(x1ha, k1ha) = Heavy_ball(f1, x1, N, ARMIJO, heavy_ball_weight, true)\n",
    "\n",
    "\n",
    "# Task (2)\n",
    "# (2) (cost and iterations)\n",
    "\n",
    "#(f2gg, k2gg) = Gradient(f2, x2, N, GOLDEN, false)\n",
    "#(f2ga, k2ga) = Gradient(f2, x2, N, ARMIJO, false)\n",
    "#(f2hg, k2hg) = Heavy_ball(f2, x2, N, GOLDEN, heavy_ball_weight, false)\n",
    "#(f2ha, k2ha) = Heavy_ball(f2, x2, N, ARMIJO, heavy_ball_weight, false)\n",
    "#(f2ng, k2ng) = Newton(f2, x2, N, GOLDEN, false)\n",
    "#(f2na, k2na) = Newton(f2, x2, N, ARMIJO, false)\n",
    "#(f2bg, k2bg) = BFGS(f2, x2, N, GOLDEN, false)\n",
    "#(f2ba, k2ba) = BFGS(f2, x2, N, ARMIJO, false)\n",
    "\n",
    "# (2) (history and iterations)\n",
    "\n",
    "#(x2gg, k2gg) = Gradient(f2, x2, N, GOLDEN, true)\n",
    "#(x2ga, k2ga) = Gradient(f2, x2, N, ARMIJO, true)\n",
    "#(x2hg, k2hg) = Heavy_ball(f2, x2, N, GOLDEN, heavy_ball_weight, true)\n",
    "#(x2ha, k2ha) = Heavy_ball(f2, x2, N, ARMIJO, heavy_ball_weight, true)\n",
    "#(x2ng, k2ng) = Newton(f2, x2, N, GOLDEN, true)\n",
    "#(x2na, k2na) = Newton(f2, x2, N, ARMIJO, true)\n",
    "#(x2bg, k2bg) = BFGS(f2, x2, N, GOLDEN, true)\n",
    "#(x2ba, k2ba) = BFGS(f2, x2, N, ARMIJO, true)\n",
    "\n",
    "\n",
    "# Task (3)\n",
    "# (3) (cost and iterations)\n",
    "\n",
    "#(f3gg, k3gg) = Gradient(f3, x3, N, GOLDEN, false)\n",
    "#(f3ga, k3ga) = Gradient(f3, x3, N, ARMIJO, false)\n",
    "#(f3hg, k3hg) = Heavy_ball(f3, x3, N, GOLDEN, heavy_ball_weight, false)\n",
    "#(f3ha, k3ha) = Heavy_ball(f3, x3, N, ARMIJO, heavy_ball_weight, false)\n",
    "#(f3ng, k3ng) = Newton(f3, x3, N, GOLDEN, false)\n",
    "#(f3na, k3na) = Newton(f3, x3, N, ARMIJO, false)\n",
    "#(f3bg, k3bg) = BFGS(f3, x3, N, GOLDEN, false)\n",
    "#(f3ba, k3ba) = BFGS(f3, x3, N, ARMIJO, false)\n",
    "\n",
    "# (3) (history and iterations)\n",
    "\n",
    "#(f3gg, k3gg) = Gradient(f3, x3, N, GOLDEN, true)\n",
    "#(f3ga, k3ga) = Gradient(f3, x3, N, ARMIJO, true)\n",
    "#(f3hg, k3hg) = Heavy_ball(f3, x3, N, GOLDEN, heavy_ball_weight, true)\n",
    "#(f3ha, k3ha) = Heavy_ball(f3, x3, N, ARMIJO, heavy_ball_weight, true)\n",
    "#(f3ng, k3ng) = Newton(f3, x3, N, GOLDEN, true)\n",
    "#(f3na, k3na) = Newton(f3, x3, N, ARMIJO, true)\n",
    "#(f3bg, k3bg) = BFGS(f3, x3, N, GOLDEN, true)\n",
    "#(f3ba, k3ba) = BFGS(f3, x3, N, ARMIJO, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance profiles\n",
    "\n",
    "The remaining code will generate the instances and performance profiles for Task (4) using your implementations for the methods and the parameters set in the parameter list above. No implementation is required from here onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using Random\n",
    "# pyplot()\n",
    "\n",
    "## Generate a random symmetric positive definite matrix\n",
    "## A ∈ ℜⁿˣⁿ and a random vector b ∈ ℜⁿ\n",
    "function generate_problem_data(n::Int, δ::Float64)\n",
    "    A = randn(n,n)                # Create random matrix\n",
    "    A = (A + A')/2                # Make A symmetric\n",
    "    if isposdef(A) == false       # Check if A is PD\n",
    "        λᵢ = eigmin(A)            # Minimum eigenvalue\n",
    "        A = A + (abs(λᵢ) + δ)*I   # Add λᵢ + δ to diagonal elements\n",
    "    end\n",
    "    @assert(isposdef(A))          # Final PD test\n",
    "    b = randn(n)                  # Create random vector b\n",
    "    return (A,b)                  # Reulting matrix A is PD\n",
    "end\n",
    "\n",
    "\n",
    "## Generate k test instances of dimension n\n",
    "function generate_instances(k::Int, n::Int, δ::StepRangeLen)\n",
    "    A = Dict{Int,Matrix{Float64}}()   # Store matrices A\n",
    "    b = Dict{Int,Vector{Float64}}()   # Store vectors  b\n",
    "    for i = 1:k\n",
    "        ## NOTE: Change δ between, e.g., δ ∈ [0.01, 1] to get different\n",
    "        ##       condition numbers for matrix A\n",
    "        (A[i], b[i]) = generate_problem_data(n, δ[i])\n",
    "    end\n",
    "    return (A, b)\n",
    "end\n",
    "\n",
    "Random.seed!(0)                             # Control randomness\n",
    "k = 100                                     # Number of intances to generate\n",
    "n = 150                                     # Dimension of PD matrix A ∈ ℜⁿˣⁿ\n",
    "δ1 = range(0.05, length = k, step = 0.05)   # Moderate condition numbers for matrices A\n",
    "δ2 = range(0.01, length = k, step = 0.01)   # Larger condition numbers for matrices A\n",
    "\n",
    "\n",
    "## Generate problem data with δ1 and δ2\n",
    "(A1, b1) = generate_instances(k, n, δ1)\n",
    "(A2, b2) = generate_instances(k, n, δ2)\n",
    "\n",
    "\n",
    "## Function to minimize with two different data\n",
    "f1(x,i) = (1/2)*dot(x, A1[i]*x) - dot(b1[i], x)\n",
    "f2(x,i) = (1/2)*dot(x, A2[i]*x) - dot(b2[i], x)\n",
    "\n",
    "## Optimal solution costs\n",
    "fopt = zeros(k,2)\n",
    "for i = 1:k\n",
    "    x1 = A1[i]\\b1[i]\n",
    "    x2 = A2[i]\\b2[i]\n",
    "    fopt[i,1] = f1(x1,i)\n",
    "    fopt[i,2] = f2(x2,i)\n",
    "end\n",
    "\n",
    "###### Preallocate data #######\n",
    "\n",
    "# Solution costs\n",
    "fval_gradient_golden   = zeros(k, 2)\n",
    "fval_gradient_armijo   = zeros(k, 2)\n",
    "fval_heavy_ball_golden = zeros(k, 2)\n",
    "fval_heavy_ball_armijo = zeros(k, 2)\n",
    "fval_newton_golden     = zeros(k, 2)\n",
    "fval_newton_armijo     = zeros(k, 2)\n",
    "fval_bfgs_golden       = zeros(k, 2)\n",
    "fval_bfgs_armijo       = zeros(k, 2)\n",
    "\n",
    "# Solution times\n",
    "time_gradient_golden   = zeros(k, 2)\n",
    "time_gradient_armijo   = zeros(k, 2)\n",
    "time_heavy_ball_golden = zeros(k, 2)\n",
    "time_heavy_ball_armijo = zeros(k, 2)\n",
    "time_newton_golden     = zeros(k, 2)\n",
    "time_newton_armijo     = zeros(k, 2)\n",
    "time_bfgs_golden       = zeros(k, 2)\n",
    "time_bfgs_armijo       = zeros(k, 2)\n",
    "\n",
    "# Number of iterations\n",
    "iter_gradient_golden   = zeros(Int, k, 2)\n",
    "iter_gradient_armijo   = zeros(Int, k, 2)\n",
    "iter_heavy_ball_golden = zeros(Int, k, 2)\n",
    "iter_heavy_ball_armijo = zeros(Int, k, 2)\n",
    "iter_newton_golden     = zeros(Int, k, 2)\n",
    "iter_newton_armijo     = zeros(Int, k, 2)\n",
    "iter_bfgs_golden       = zeros(Int, k, 2)\n",
    "iter_bfgs_armijo       = zeros(Int, k, 2)\n",
    "\n",
    "# Solution status\n",
    "stat_gradient_golden   = fill(false, k, 2)\n",
    "stat_gradient_armijo   = fill(false, k, 2)\n",
    "stat_heavy_ball_golden = fill(false, k, 2)\n",
    "stat_heavy_ball_armijo = fill(false, k, 2)\n",
    "stat_newton_golden     = fill(false, k, 2)\n",
    "stat_newton_armijo     = fill(false, k, 2)\n",
    "stat_bfgs_golden       = fill(false, k, 2)\n",
    "stat_bfgs_armijo       = fill(false, k, 2)\n",
    "\n",
    "ns  = 8                          # Number of solvers (methods) to compare\n",
    "np  = k                          # Number of problems to solve\n",
    "computing_time = zeros(np,ns,2)  # Computing times for each problem/method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x₀  = ones(n)   # Starting point\n",
    "tini = time()   # Timekeep start\n",
    "\n",
    "## Go through all instances for both sets of data\n",
    "for j = 1:2\n",
    "    \n",
    "    for i = 1:k\n",
    "    \n",
    "        ## Function to minimize\n",
    "        g1(x) = f1(x,i)\n",
    "        g2(x) = f2(x,i)\n",
    "    \n",
    "        ## Gradient + Golden\n",
    "        starttime = time()                      # Start timer\n",
    "        if j == 1\n",
    "            (fvalue, numiter) = Gradient(g1, x₀, N, GOLDEN, false)\n",
    "        else\n",
    "            (fvalue, numiter) = Gradient(g2, x₀, N, GOLDEN, false)\n",
    "        end\n",
    "        soltime = time() - starttime              # Solution time\n",
    "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
    "        fval_gradient_golden[i, j] = fvalue       # Objective value\n",
    "        time_gradient_golden[i, j] = soltime      # Solution time\n",
    "        iter_gradient_golden[i, j] = numiter      # Iteration count\n",
    "        stat_gradient_golden[i, j] = status       # Solution status\n",
    "        ## Set solution time accordingly\n",
    "        status == true ? computing_time[i, 1, j] = soltime : computing_time[i, 1, j] = Inf    \n",
    "    \n",
    "        ## Gradient + Armijo\n",
    "        starttime = time()                      # Start timer\n",
    "        if j == 1\n",
    "            (fvalue, numiter) = Gradient(g1, x₀, N, ARMIJO, false)\n",
    "        else\n",
    "            (fvalue, numiter) = Gradient(g2, x₀, N, ARMIJO, false)\n",
    "        end\n",
    "        soltime = time() - starttime            # Solution time\n",
    "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
    "        fval_gradient_armijo[i, j] = fvalue     # Objective value\n",
    "        time_gradient_armijo[i, j] = soltime    # Solution time\n",
    "        iter_gradient_armijo[i, j] = numiter    # Iteration count\n",
    "        stat_gradient_armijo[i, j] = status     # Solution status\n",
    "        ## Set solution time accordingly\n",
    "        status == true ? computing_time[i, 2, j] = soltime : computing_time[i, 2, j] = Inf  \n",
    "    \n",
    "        ## Heavy ball + Golden\n",
    "        starttime = time()                      # Start timer\n",
    "        if j == 1\n",
    "            (fvalue, numiter) = Heavy_ball(g1, x₀, N, GOLDEN, heavy_ball_weight, false)\n",
    "        else\n",
    "            (fvalue, numiter) = Heavy_ball(g2, x₀, N, GOLDEN, heavy_ball_weight, false)\n",
    "        end\n",
    "        soltime = time() - starttime              # Solution time\n",
    "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
    "        fval_heavy_ball_golden[i, j] = fvalue     # Objective value\n",
    "        time_heavy_ball_golden[i, j] = soltime    # Solution time\n",
    "        iter_heavy_ball_golden[i, j] = numiter    # Iteration count\n",
    "        stat_heavy_ball_golden[i, j] = status     # Solution status\n",
    "        ## Set solution time accordingly\n",
    "        status == true ? computing_time[i, 3, j] = soltime : computing_time[i, 3, j] = Inf  \n",
    "    \n",
    "        ## Heavy ball + Armijo\n",
    "        starttime = time()                        # Start timer\n",
    "        if j == 1\n",
    "            (fvalue, numiter) = Heavy_ball(g1, x₀, N, ARMIJO, heavy_ball_weight, false)\n",
    "        else\n",
    "            (fvalue, numiter) = Heavy_ball(g2, x₀, N, ARMIJO, heavy_ball_weight, false)\n",
    "        end\n",
    "        soltime = time() - starttime              # Solution time\n",
    "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
    "        fval_heavy_ball_armijo[i, j] = fvalue     # Objective value\n",
    "        time_heavy_ball_armijo[i, j] = soltime    # Solution time\n",
    "        iter_heavy_ball_armijo[i, j] = numiter    # Iteration count\n",
    "        stat_heavy_ball_armijo[i, j] = status     # Solution status\n",
    "        ## Set solution time accordingly\n",
    "        status == true ? computing_time[i, 4, j] = soltime : computing_time[i, 4, j] = Inf  \n",
    "\n",
    "        ## Newton + Golden\n",
    "        starttime = time()                        # Start timer\n",
    "        if j == 1\n",
    "            (fvalue, numiter) = Newton(g1, x₀, N, GOLDEN, false)\n",
    "        else\n",
    "            (fvalue, numiter) = Newton(g2, x₀, N, GOLDEN, false)\n",
    "        end\n",
    "        soltime = time() - starttime              # Solution time\n",
    "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
    "        fval_newton_golden[i, j] = fvalue         # Objective value\n",
    "        time_newton_golden[i, j] = soltime        # Solution time\n",
    "        iter_newton_golden[i, j] = numiter        # Iteration count\n",
    "        stat_newton_golden[i, j] = status         # Solution status\n",
    "        ## Set solution time accordingly\n",
    "        status == true ? computing_time[i, 5, j] = soltime : computing_time[i, 5, j] = Inf\n",
    "    \n",
    "        ## Newton + Armijo\n",
    "        starttime = time()                        # Start timer\n",
    "        if j == 1\n",
    "            (fvalue, numiter) = Newton(g1, x₀, N, ARMIJO, false)\n",
    "        else\n",
    "            (fvalue, numiter) = Newton(g2, x₀, N, ARMIJO, false)\n",
    "        end\n",
    "        soltime = time() - starttime              # Solution time\n",
    "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
    "        fval_newton_armijo[i, j] = fvalue         # Objective value\n",
    "        time_newton_armijo[i, j] = soltime        # Solution time\n",
    "        iter_newton_armijo[i, j] = numiter        # Iteration count\n",
    "        stat_newton_armijo[i, j] = status         # Solution status\n",
    "        ## Set solution time accordingly\n",
    "        status == true ? computing_time[i, 6, j] = soltime : computing_time[i, 6, j] = Inf    \n",
    "        \n",
    "        ## BFGS + Golden\n",
    "        starttime = time()                        # Start timer\n",
    "        if j == 1\n",
    "            (fvalue, numiter) = BFGS(g1, x₀, N, GOLDEN, false)\n",
    "        else\n",
    "            (fvalue, numiter) = BFGS(g2, x₀, N, GOLDEN, false)\n",
    "        end\n",
    "        soltime = time() - starttime              # Solution time\n",
    "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
    "        fval_bfgs_golden[i, j] = fvalue           # Objective value\n",
    "        time_bfgs_golden[i, j] = soltime          # Solution time\n",
    "        iter_bfgs_golden[i, j] = numiter          # Iteration count\n",
    "        stat_bfgs_golden[i, j] = status           # Solution status\n",
    "        ## Set solution time accordingly\n",
    "        status == true ? computing_time[i, 7, j] = soltime : computing_time[i, 7, j] = Inf\n",
    "    \n",
    "        ## BFGS + Armijo\n",
    "        starttime = time()                        # Start timer\n",
    "        if j == 1\n",
    "            (fvalue, numiter) = BFGS(g1, x₀, N, ARMIJO, false)\n",
    "        else\n",
    "            (fvalue, numiter) = BFGS(g2, x₀, N, ARMIJO, false)\n",
    "        end\n",
    "        soltime = time() - starttime              # Solution time\n",
    "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
    "        fval_bfgs_armijo[i, j] = fvalue           # Objective value\n",
    "        time_bfgs_armijo[i, j] = soltime          # Solution time\n",
    "        iter_bfgs_armijo[i, j] = numiter          # Iteration count\n",
    "        stat_bfgs_armijo[i, j] = status           # Solution status\n",
    "        ## Set solution time accordingly\n",
    "        status == true ? computing_time[i, 8, j] = soltime : computing_time[i, 8, j] = Inf\n",
    "    end    \n",
    "end\n",
    "tend = time() - tini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot performance profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j = 1:2\n",
    "    ###### Plot performance profiles ######\n",
    "    computing_time_min = minimum(computing_time[:, :, j], dims = 2)    # Minimum time for each instance\n",
    "    performance_ratios = computing_time[:, :, j] ./ computing_time_min # Compute performance ratios\n",
    "\n",
    "    τ = sort(unique(performance_ratios))  # Sort the performance ratios in increasing order\n",
    "    τ[end] == Inf && pop!(τ)  # Remove the Inf element if it exists\n",
    "\n",
    "    ns = 8                    # Number of solvers\n",
    "    np = k                    # Number of problems\n",
    "\n",
    "    ρS = Dict()               # Compute cumulative distribution functions\n",
    "    for i = 1:ns              # for performance ratios\n",
    "        ρS[i] = [sum(performance_ratios[:,i] .<= τi) / np for τi in τ]\n",
    "    end\n",
    "\n",
    "    # Plot performance profiles\n",
    "    labels = [\"Gradient (Exact)\", \"Gradient (Armijo)\", \"Heavy ball (Exact)\", \"Heavy ball (Armijo)\",\n",
    "              \"Newton (Exact)\", \"Newton (Armijo)\", \"BFGS (Exact)\", \"BFGS (Armijo)\"]\n",
    "\n",
    "    styles = [:solid, :dash, :dot, :dashdot, :solid, :dash, :solid, :dash,]\n",
    "    plot(xscale = :log2,  \n",
    "         yscale = :none,\n",
    "         xlim   = (1, maximum(τ)),\n",
    "         ylim   = (0, 1),\n",
    "         xlabel = \"τ\",\n",
    "         ylabel = \"P(performance_ratios ≤ τ : 1 ≤ s ≤ np)\",\n",
    "         title  = \"Perfomance plot (condition $j)\",\n",
    "         xformatter = xi -> xi,\n",
    "         yticks = 0.0:0.1:1.0,\n",
    "         size   = (1200,800),\n",
    "         reuse  = false,\n",
    "         tickfontsize   = 8,\n",
    "         legendfontsize = 10,\n",
    "         guidefontsize  = 10,\n",
    "         grid = true)\n",
    "    for i = 1:ns\n",
    "      plot!(τ, ρS[i], label = labels[i], seriestype = :steppre, linewidth = 2, line = styles[i])\n",
    "    end\n",
    "    savefig(\"performance_plot_$j.pdf\")\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
