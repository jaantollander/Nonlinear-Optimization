{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3.2\n",
    "- Jaan Tollander de Balsch\n",
    "- 452056"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "H (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ForwardDiff\n",
    "using LinearAlgebra\n",
    "\n",
    "## Functions to compute gradient and hessian. You can use these to complete parts of the code.\n",
    "∇(f,x) = ForwardDiff.gradient(f, x)\n",
    "H(f,x) = ForwardDiff.hessian(f, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line search : Golden Section method\n",
    "\n",
    "### Input parameters: \n",
    "\n",
    "$\\begin{align*}\n",
    "&\\theta: \\text{ line search function}\\\\\n",
    "&a: \\text{ initial lower bound}\\\\ \n",
    "&b: \\text{ initial upper bound}\n",
    "\\end{align*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "golden_ls (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  Θ: line search function\n",
    "##  a: initial lower bound\n",
    "##  b: initial upper bound\n",
    "function golden_ls(θ, a, b)\n",
    "    l  = 1e-10                    # Tolerance (length of uncertainty)\n",
    "    α  = 1/Base.MathConstants.φ   # φ = golden ratio. Here α ≈ 0.618\n",
    "    \n",
    "    λ  = a + (1-α)*(b - a)        # NOTE: We do not need to index a, b, λ, and μ like in the lecture 5 pseudocode\n",
    "    μ  = a + α*(b - a)            #       Instead, we can keep reusing and updating the same variables for notational convenience\n",
    "    \n",
    "    θμ = θ(a + α*(b - a))         # Use this variable to compute function values Θ(μₖ₊₁) as in the pseudocode of Lecture 5\n",
    "    θλ = θ(a + (1 - α)*(b - a))   # Use this variable to compute function values Θ(λₖ₊₁) as in the pseudocode of Lecture 5\n",
    "\n",
    "    \n",
    "    ## TODO: Implement what should be inside the while loop of Golden Section method\n",
    "    while b - a > l\n",
    "        if θλ > θμ\n",
    "            a = λ\n",
    "            λ = μ\n",
    "            μ = a + α*(b - a)\n",
    "            θλ = θμ\n",
    "            θμ = θ(μ)\n",
    "        else\n",
    "            b = μ\n",
    "            μ = λ\n",
    "            λ = a + (1 - α)*(b - a)\n",
    "            θμ = θλ\n",
    "            θλ = θ(λ)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return (a + b)/2              # Finally, the function returns the center point of the final interval\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "### Parameters\n",
    "\n",
    "$\\begin{align*}\n",
    "f:& \\text{ function to minimize}\\\\\n",
    "x:& \\text{ empty solution vector with starting point } x[1,:] = [x_1^0, x_2^0]\\\\ \n",
    "N:& \\text{ maximum number of iterations}\n",
    "\\end{align*}$\n",
    "\n",
    "### Keyword arguments\n",
    "\n",
    "$\\begin{align*}\n",
    "\\epsilon:& \\text{ stopping criterion tolerance}\\\\\n",
    "a:& \\text{ initial lower bound in Golden Section line search}\\\\\n",
    "b:& \\text{ initial upper bound in Golden Section line search}\n",
    "\\end{align*}$\n",
    "\n",
    "### Output:\n",
    "\n",
    "$\\begin{align*}\n",
    "x:& \\text{ solution vector containing points } x^k \\text{ at each iteration } 1,\\dots,k\\\\\n",
    "f(x):& \\text{ objective function values at each iteration}\\\\\n",
    "k:& \\text{ total number of iterations}\n",
    "\\end{align*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gradient (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Gradient Descent \n",
    "function Gradient(f, x, N; ϵ = 1e-6, a = a₀, b = b₀)\n",
    "    \n",
    "    for k = 1:N-1                   # Main iteration loop\n",
    "        \n",
    "        ∇f = ∇(f, x[k,:])          # Gradient at iteration k\n",
    "        \n",
    "        if norm(∇f) < ϵ                              # Stopping condition: norm of the gradient < tolerance\n",
    "            return (x[1:k,:], f.(x[i,:] for i = 1:k), k-1) # Return iteration points, function values, and number of iterations\n",
    "        end\n",
    "        \n",
    "        ## TODO: set the Gradient Descent direction\n",
    "        d = - ∇f / norm(∇f)\n",
    "        \n",
    "        θ(λ) = f(x[k,:] + λ*d)      # Define the line search function \n",
    "        λ    = golden_ls(θ, a, b)   # Call Golden Section method to compute optimal step size λ  \n",
    "\n",
    "        ## TODO: Update the solution x[k+1,:] at this iteration accordingly\n",
    "        x[k+1,:] = x[k,:] + λ*d                 \n",
    "        \n",
    "    end\n",
    "    \n",
    "    return (x, f.(x[i,:] for i = 1:N), N)           # Return iteration points, function values, and number of iterations\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's method\n",
    "\n",
    "### Parameters\n",
    "\n",
    "$\\begin{align*}\n",
    "f:& \\text{ function to minimize}\\\\\n",
    "x:& \\text{ empty solution vector with starting point } x[1,:] = [x_1^0, x_2^0]\\\\ \n",
    "N:& \\text{ maximum number of iterations}\n",
    "\\end{align*}$\n",
    " \n",
    "### Keyword arguments\n",
    "\n",
    "$\\begin{align*}\n",
    "\\epsilon:& \\text{ stopping criterion tolerance}\\\\\n",
    "a:& \\text{ initial lower bound in Golden Section line search}\\\\\n",
    "b:& \\text{ initial upper bound in Golden Section line search}\n",
    "\\end{align*}$\n",
    "\n",
    "### Output:\n",
    "\n",
    "$\\begin{align*}\n",
    "x:& \\text{ solution vector containing points } x^k \\text{ at each iteration } 1,\\dots,k\\\\\n",
    "f(x):& \\text{ objective function values at each iteration}\\\\\n",
    "k:& \\text{ total number of iterations}\n",
    "\\end{align*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Newton (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Newton(f, x, N; ϵ = 1e-6, a = a₀, b = b₀)\n",
    "    \n",
    "    for k = 1:N-1                    # Main iteration loop\n",
    "        \n",
    "        ∇f = ∇(f, x[k,:])         # Gradient at iteration k\n",
    "        \n",
    "        if norm(∇f) < ϵ                              # Stopping condition: norm of the gradient < tolerance\n",
    "            return (x[1:k,:], f.(x[i,:] for i = 1:k), k-1)  # Return iteration points, function values, and number of iterations\n",
    "        end\n",
    "        \n",
    "        ## TODO: Update the newton direction\n",
    "        # d = -inv(H(f, x[k,:]))*∇f\n",
    "        d = -H(f, x[k,:])\\∇f\n",
    "        \n",
    "        θ(λ) = f(x[k,:] + λ*d)     # Define the line search function \n",
    "        λ = golden_ls(θ, a, b)    # Call Golden Section method to compute optimal step size λ  \n",
    "\n",
    "        ## TODO: Update the solution x[k+1,:] at this iteration accordingly\n",
    "        x[k+1,:] = x[k,:] + λ*d              \n",
    "    \n",
    "    end\n",
    "    \n",
    "    return (x,  f.(x[i,:] for i = 1:N), N)          # Return iteration points, function values, and number of iterations\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate Gradient\n",
    "\n",
    "### Parameters\n",
    "\n",
    "$\\begin{align*}\n",
    "f:& \\text{ function to minimize}\\\\\n",
    "x:& \\text{ empty solution vector with starting point } x[1,:] = [x_1^0, x_2^0]\\\\ \n",
    "N:& \\text{ maximum number of iterations}\n",
    "\\end{align*}$\n",
    "\n",
    "### Keyword arguments\n",
    "\n",
    "$\\begin{align*}\n",
    "\\epsilon:& \\text{ stopping criterion tolerance}\\\\\n",
    "a:& \\text{ initial lower bound in Golden Section line search}\\\\\n",
    "b:& \\text{ initial upper bound in Golden Section line search}\n",
    "\\end{align*}$\n",
    "\n",
    "### Output:\n",
    "\n",
    "$\\begin{align*}\n",
    "x:& \\text{ solution vector containing points } x^k \\text{ at each iteration } 1,\\dots,k\\\\\n",
    "f(x):& \\text{ objective function values at each iteration}\\\\\n",
    "k:& \\text{ total number of iterations}\n",
    "\\end{align*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conjugate_Gradient (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Conjugate_Gradient(f, x, N; ϵ = 1e-6, a = a₀, b = b₀)\n",
    "      \n",
    "    α = 0              # Coefficient for Fletcher-Reeves update\n",
    "    k = 1              # Iteration number  \n",
    "    n = size(x, 2)     # Dimension of x\n",
    "    d = -∇(f, x[1,:])  # Initial direction vector\n",
    "    \n",
    "    while k <= N-1     # Go through max iterations N and return if at optimum \n",
    "        \n",
    "        for j = 1:n    # Go through each element of x. NOTE: We do not need to use y variables. Instead, \n",
    "                       # we can use the empty values in the x variable vector \n",
    "\n",
    "            θ(λ) = f(x[k,:] + λ*d)   # Define the line search function \n",
    "            λ = golden_ls(θ, a, b)   # Call Golden Section method to compute optimal step size λ  \n",
    "            \n",
    "            ## TODO: Update the value of x[k+1,:] accordingly\n",
    "            x[k+1,:] = x[k,:] + λ*d\n",
    "           \n",
    "            ## TODO: Compute value of α using the Fletcher-Reeves update formula\n",
    "            α = norm(∇(f, x[k+1,:]))^2 / norm(∇(f, x[k,:]))^2\n",
    "            ## TODO: Set the direction vector accordingly\n",
    "            d = -∇(f, x[k+1,:]) + α*d\n",
    "            \n",
    "            k = k + 1   # Update number of iterations for the y values (here we use x vector instead as mentioned earlier)\n",
    "            \n",
    "        end\n",
    "        \n",
    "        d = -∇(f, x[k,:])\n",
    "        \n",
    "        if norm(d) < ϵ                               # Stopping condition: norm of the gradient < tolerance\n",
    "            return (x[1:k,:], f.(x[i,:] for i = 1:k), k-1)  # Return iteration points, function values, and number of iterations\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    \n",
    "    return (x, f.(x[i,:] for i = 1:N), N)                               # Return iteration points, function values, and number of iterations\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "g (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x) = 0.26*(x[1]^2 + x[2]^2) - 0.48*x[1]*x[2] \n",
    "g(x) = exp(x[1] + 3*x[2] - 0.1) + exp(x[1] - 3*x[2] - 0.1) + exp(-x[1] - 0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Function 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([7.0 3.0; 4.77889 4.81727; … ; 9.29564e-6 3.98399e-6; 6.3462e-6 6.39716e-6], [5.0, 0.921232, 0.169733, 0.0312727, 0.00576185, 0.0010616, 0.000195594, 3.60372e-5, 6.63968e-6, 1.22333e-6, 2.25393e-7, 4.15278e-8, 7.65128e-9, 1.40971e-9, 2.59722e-10, 4.78508e-11, 8.81689e-12, 1.62458e-12], 17)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N  = 10000\n",
    "x  = zeros(N,2)\n",
    "a₀ = -25.0\n",
    "b₀ =  25.0\n",
    "x[1,:] = [7.0, 3.0]\n",
    "(xg, fg, kg) = Gradient(f, x, N)\n",
    "## NOTE: The optimal solution is (0,0) and the optimal cost is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([7.0 3.0; 1.79593e-10 7.69704e-11], [5.0, 3.29111e-21], 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N  = 10000\n",
    "x  = zeros(N,2)\n",
    "a₀ = -25.0\n",
    "b₀ =  25.0\n",
    "x[1,:] = [7.0, 3.0]\n",
    "(xn, fn, kn) = Newton(f, x, N)\n",
    "## NOTE: The optimal solution is (0,0) and the optimal cost is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([7.0 3.0; 4.77889 4.81727; -6.2293e-8 5.09639e-8], [5.0, 0.921232, 3.20806e-15], 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N  = 10000\n",
    "x  = zeros(N,2)\n",
    "a₀ = -25.0\n",
    "b₀ =  25.0\n",
    "x[1,:] = [7.0, 3.0]\n",
    "(xc, fc, kc) = Conjugate_Gradient(f, x, N)\n",
    "## NOTE: The optimal solution is (0,0) and the optimal cost is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the Newton's method relies on the second order approximation (Hessian) it only requires one iterations for solving (minimize) a second order function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Function 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-4.0 -2.0; -0.0158711 -0.12925; … ; -0.346573 -2.03498e-7; -0.346573 2.69534e-8], [56.0884, 2.83608, 2.64966, 2.59035, 2.56854, 2.5621, 2.56009, 2.55951, 2.55934, 2.55929  …  2.55927, 2.55927, 2.55927, 2.55927, 2.55927, 2.55927, 2.55927, 2.55927, 2.55927, 2.55927], 24)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N  = 10000\n",
    "x  = zeros(N,2)\n",
    "a₀ = -25.0\n",
    "b₀ =  25.0\n",
    "x[1,:] = [-4.0, -2.0]\n",
    "(xg, fg, kg) = Gradient(g, x, N)\n",
    "## NOTE: The optimal solution is approximately (-0.346574, 0.0) with cost 2.55927"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-4.0 -2.0; -0.742904 0.171378; … ; -0.346958 0.000193064; -0.346574 -1.02522e-8], [56.0884, 2.87923, 2.56265, 2.55927, 2.55927], 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N  = 10000\n",
    "x  = zeros(N,2)\n",
    "a₀ = -25.0\n",
    "b₀ =  25.0\n",
    "x[1,:] = [-4.0, -2.0]\n",
    "(xn, fn, kn) = Newton(g, x, N)\n",
    "## NOTE: The optimal solution is approximately (-0.346574, 0.0) with cost 2.55927"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-4.0 -2.0; -0.0158711 -0.12925; … ; -0.346572 -3.72666e-8; -0.346574 -1.94385e-8], [56.0884, 2.83608, 2.65576, 2.5884, 2.55938, 2.5593, 2.55927, 2.55927, 2.55927], 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N  = 10000\n",
    "x  = zeros(N,2)\n",
    "a₀ =  -25.0\n",
    "b₀ =   25.0\n",
    "x[1,:] = [-4.0, -2.0]\n",
    "(xc, fc, kc) = Conjugate_Gradient(g, x, N)\n",
    "## NOTE: The optimal solution is approximately (-0.346574, 0.0) with cost 2.55927"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
